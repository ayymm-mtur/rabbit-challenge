{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. サポートベクターマシン\n",
    "\n",
    "もともとは入力データを二クラスに分類する問題を対象にしていたが、今では回帰問題、教師なし学習に用いられる。\n",
    "\n",
    "### 決定関数と分類境界\n",
    "入力xが与えられたときに、入力がどちらのクラスに属するのか判断するときに決定関数を用いる\n",
    "$$\n",
    "f(x) = \\omega^T X + b \\label{a}\\tag{1}\n",
    "$$\n",
    "\n",
    "決定関数の出力を符号関数に通すことにより、どちらのクラスに属するのか決定する\n",
    "$$\n",
    "y = sgn f(x) =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "+1 & (f(x)>0) \\\\\n",
    "-1 & (f(x)<0)\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\label{b}\\tag{2}\n",
    "$$\n",
    "\n",
    "### 線形サポートベクトル分類(ハードマシン)\n",
    "入力データがすべて$(\\ref{a})$で分類できるとするとき、決定関数の予測値sgnf(x)と訓練データ$y_i$の値に関して次式が成立する\n",
    "$$\n",
    "y_i f(x_i) > 0 \\ (i=1,2,\\cdots,n) \\quad \\label{c}\\tag{3}\n",
    "$$\n",
    "$(\\ref{c})$式が成り立つ場合に、分類境界を決定する方法を考える。<br>\n",
    "一般に訓練データを分類する境界は複数存在するが、それぞれのクラスのデータが境界からなるべく離れるようにしたい。<br>\n",
    "分類境界がどのていど離れているかをマージンといい、なるべく大きいマージンを探すことをマージン最大化という。<br>\n",
    "\n",
    "分類境から最も近くにあるデータ$x_i$との距離は\n",
    "$$\n",
    "\\frac{|f(x_i)|}{\\|w\\|}=\\frac{|w^T x_i +b|}{\\|w\\|} \\label{d}\\tag{4}\n",
    "$$\n",
    "$\\|w\\|$はL2ノルムで\n",
    "$$\n",
    "\\|w\\|=\\sqrt{w_{1}^2+w_{2}^2+\\cdots+w_{n}^2} \\label{e}\\tag{5}\n",
    "$$\n",
    "$(\\ref{e})$は次にように式変形できる\n",
    "$$\n",
    "\\frac{|w^T x_i +b|}{\\|w\\|} = \\frac{y_i[w^T x_i +b]}{\\|w\\|} \\label{f}\\tag{6}\n",
    "$$\n",
    "分類境界と境界から最も近くにあるデータ$x_i$との距離は\n",
    "$$\n",
    "\\min_{i} \\frac{y_i[w^T x_i +b]}{\\|w\\|} = \\frac{1}{\\|w\\|}\\min_{i} [v] \\equiv \\frac{M(w,b)}{\\|w\\|} \\label{g}\\tag{7}\n",
    "$$\n",
    "目的はマージンの最大化なので、目的関数は\n",
    "$$\n",
    "\\max_{w,b} \\left[ \\min_{i} \\frac{y_i[w^T x_i +b]}{\\|w\\|} \\right] = \\max_{w,b} \\frac{M(w,b)}{\\|w\\|} \\label{h}\\tag{8}\n",
    "$$\n",
    "$(\\ref{g})$より、すべての$i$に対して次式が成り立つ。これを制約条件という。\n",
    "$$\n",
    "y_i[w^T x_i +b] \\geq M(w,b) \\label{i}\\tag{9}\n",
    "$$\n",
    "なお、$\\tilde{w}$と$\\tilde{b}$を導入して変形すると\n",
    "$$\n",
    "\\tilde{w} = \\frac{w}{M(w,b)} \\label{j}\\tag{10}\n",
    "$$\n",
    "$$\n",
    "\\tilde{b} = \\frac{b}{M(w,b)} \\label{k}\\tag{11}\n",
    "$$\n",
    "$$\n",
    "y_i \\left[\\tilde{w}^T x_i + \\tilde{b} \\right] \\geq 1 \\label{l}\\tag{12}\n",
    "$$\n",
    "$\\frac{1}{\\|w\\|}$の最大化は$\\|w\\|$の最小化と等価であるから、目的関数は扱いやすい次の形に変形できる。\n",
    "$$\n",
    "\\min_{w,b}\\frac{1}{2}{\\|w\\|}^2 \\label{m}\\tag{13}\n",
    "$$\n",
    "\n",
    "分離可能性を仮定しているSV分類のことを一般にハードマージンという。\n",
    "\n",
    "### 線形サポートベクトル分類\n",
    "SV分類を分離可能でないデータに適用できるように拡張したものをソフトマージンという。\n",
    "$(\\ref{l})$の条件を、$\\xi_i\\geq0(i=1,2,\\cdots,n)$を導入し次のように変更する。\n",
    "$$\n",
    "y_i \\left[\\tilde{w}^T x_i + \\tilde{b} \\right] \\geq 1-\\xi_i \\label{n}\\tag{14}\n",
    "$$\n",
    "$\\xi_i$はマージン内のデータや誤分類されたデータに際する誤差を表す変数である。<br>\n",
    "ソフトマージンにおける最適化問題は次のように表現される。\n",
    "$$\n",
    "\\min_{w,b,\\xi}\\left[\\frac{1}{2}{\\|w\\|}^2 +C \\sum_{i=1}^{n}\\xi_i  \\right]\\label{o}\\tag{15}\n",
    "$$\n",
    "Cを正則化係数といい、学習前に値を決めておくハイパーパラメータである。<br>\n",
    "Cが大きいほどハードマージンに近づくが、データが分離可能でない場合にCを大きくし過ぎると、<br>\n",
    "目的関数が発散して計算できなくなるかもしれない。そのため、データに合わせて適切なCを決定する必要がある。\n",
    "\n",
    "### SVMにおける双対表現\n",
    "$(\\ref{m})$や$(\\ref{o})$をSV分類の主問題という。主問題を解けば分類協会を決定できるが、主問題と等価な<br>\n",
    "双対問題を解くことが一般的である。双対問題を解くことは主問題に比べ次のメリットがある。\n",
    "- 双対問題の４法が変数を少なくできる\n",
    "- 分類境界の非線形化を考える上で双対問題の方が有利\n",
    "<br>\n",
    "\n",
    "### 双対問題の導出\n",
    "ソフトマージンの場合の最適化問題の双対問題を考える。<br>\n",
    "まず、次のラグランジュ関数を導入する\n",
    "$$\n",
    "L(w,b,\\xi,\\alpha,\\mu) = \n",
    "\\frac{1}{2}{\\|w\\|}^2 + C\\sum_{i=1}^{n}\\xi_i \n",
    "- \\sum_{i=1}^{n}\\alpha_i\\left[y_i\\left[w^Tx_i+b \\right] -1 + \\xi_i \\right] - \\sum_{i=1}^{n}\\mu_i\\xi_i \\label{p}\\tag{16}\n",
    "$$\n",
    "\n",
    "ただし、$\\alpha_i \\geq 0,\\mu_i \\geq 0 (i = 1, \\cdots,n)$\n",
    "$$\n",
    "\\alpha = (\\alpha_1,\\alpha_2,\\cdots,\\alpha_n)^T, \\mu=(\\mu_1,\\mu_2,\\cdots,\\mu_n)^T \\label{q}\\tag{17}\n",
    "$$\n",
    "\n",
    "ここでもともとの主問題にある変数$w, b, \\xi$を主変数、あらたに登場した変数$\\alpha, \\mu$を双対変数という。\n",
    "<img src=\"./image/2_6_svm_1.jpg\">\n",
    "\n",
    "### カーネルを用いた非線形分離への拡張\n",
    "入力空間でn次元であるデータをより高次のr次元の特徴空間へ変換する関数\n",
    "$$\n",
    "\\phi(x) = \n",
    "\\begin{pmatrix}\n",
    "\\phi_1(x)\\\\\n",
    "\\vdots\\\\\n",
    "\\phi_r(x)\n",
    "\\end{pmatrix} \\label{r}\\tag{25}\n",
    "$$\n",
    "この写像により、SV分類での双対問題の目的関数は、特徴空間上での目的関数に次のように変換される\n",
    "$$\n",
    "\\max_{\\alpha} \\left[ -\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j \\phi(x_i)^T \\phi(x_j) + \\sum_{i=1}^{n} \\alpha_i \\right] \\label{s}\\tag{26}\n",
    "$$\n",
    "$\\phi(x_i)^T \\phi(x_j)$の内積部分の計算量が莫大になるため、$(\\ref{s})$をそのまま解くことは困難である。<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
