{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.過学習\n",
    "\n",
    "過学習とはモデルが特定の訓練サンプルに特化してしまい、汎用性が低くなっていることである。\n",
    "過学習の原因としては\n",
    "・パラメータの数が多い\n",
    "・パラメータの数が不適切\n",
    "・ノードが多い\n",
    "などネットワークの自由度が高いことがあげられる。\n",
    "\n",
    "## 3-1. L1正則化、L2正則化\n",
    "正則化ではネットワークの自由度を抑制して、過学習を抑える。\n",
    "$$\n",
    "E_n(w) + \\frac{1}{p} \\|x\\|_p\\\\\n",
    "\\|x\\|_p = (|x_1|^p + \\cdots + |x_n|^p)^{\\frac{1}{p}}\n",
    "$$\n",
    "p=1の場合をL1正則化（ラッソ回帰）\n",
    "p=2の場合をL2正則化（リッジ回帰）と呼ぶ。\n",
    "\n",
    "p1ノルム=マンハッタン距離<br>\n",
    "p2ノルム=ユークリッド距離\n",
    "\n",
    "## 3-2. ドロップアウト\n",
    "ランダムにノードを削除して学習させること。\n",
    "メリット：同じデータを用いて、異なるモデルを学習させていると解釈できる。\n",
    "\n",
    "\n",
    "## 確認テスト\n",
    "### Q. 下図について、L1正則を表しているグラフはどちらか答えよ\n",
    "<img src=\"./image/test3-1.png\">\n",
    "\n",
    "### A. 右\n",
    "\n",
    "## 例題チャレンジ\n",
    "### 5. L2パラメータ正則化\n",
    "<img src=\"./image/test3-2.png\">\n",
    "\n",
    "### A. (4)\n",
    "L2のノルムはパラメータの二乗であるから、微分すると一次になる\n",
    "\n",
    "### 6. L1パラメータ正則化\n",
    "<img src=\"./image/test3-3.png\">\n",
    "### A.(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
