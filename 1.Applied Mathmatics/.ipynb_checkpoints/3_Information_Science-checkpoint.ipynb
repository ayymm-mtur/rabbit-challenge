{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.情報科学\n",
    "\n",
    "情報量の変化とは元の量に対するの変化の割合ではないか？\n",
    "\n",
    "もともと米粒がいっぱいある容器に米粒をひとつ足しても見た目で変化が分からないが、<br>\n",
    "米粒が一つしかない容器に一つ足すと、二つに増えたのが一目でわかる\n",
    "\n",
    "\n",
    "### 3-1. 自己情報量\n",
    "- 対数の底が2の時、単位はbit\n",
    "- 対数の底がネイピア数の時、単位はnat\n",
    "$$\n",
    "I(x)=-\\log(P(x))=\\log(W(x))\n",
    "$$\n",
    "\n",
    "### 3-2. シャノンエントロピー\n",
    "- 自己情報量の期待値\n",
    "$$\n",
    "H(x) = E(I(x))\\\\\n",
    "=-E(\\log{P(x)})\\\\\n",
    "=-\\sum(P(x\\log{P(x)}))\n",
    "$$\n",
    "\n",
    "### 3-3. カルバック・ライブラー　ダイバージェンス\n",
    "- 同じ事象・確率変数における異なる確率分布P,Qの違いを表す\n",
    "$$\n",
    "D_{KL} (P\\|Q) = E_{x～P}\\left[\\log{\\frac{P(x)}{Q(x)}}\\right] = E_{x～P}\\left[\\log{P(x)} - \\log{Q(x)}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "### 3-4. 交差エントロピー\n",
    "- KLダイバージェンスの一部を取り出したもの\n",
    "- Qの自己情報量をPの分布で平均\n",
    "$$\n",
    "H(P,Q) = H(P) + D_{KL} (P\\|Q)\\\\\n",
    "H(P,Q) = -E_{x～P}\\log{Q(x)} = =-\\sum P(x)\\log{Q(x)}\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
